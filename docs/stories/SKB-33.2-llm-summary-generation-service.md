# Story SKB-33.2: LLM Summary Generation Service

**Epic:** Epic 33 - Agent Navigation Metadata & Page Summaries
**Story ID:** SKB-33.2
**Story Points:** 5 | **Priority:** High | **Status:** Draft
**Depends On:** SKB-33.1 (summary fields must exist in Page schema)

---

## User Story

As a SymbioKnowledgeBase user, I want page summaries to be automatically generated by an LLM when I make substantial edits to a page, so that the navigation metadata stays current without manual effort.

---

## Acceptance Criteria

### Change Detection
- [ ] After each page save (PUT /api/pages/{id}/blocks), the system computes a **change ratio**:
  - `changeRatio = editDistance(oldPlainText, newPlainText) / max(len(oldPlainText), len(newPlainText))`
  - Uses a fast approximation (character-level diff, not full Levenshtein for performance)
- [ ] If `changeRatio > SUMMARY_CHANGE_THRESHOLD` (default: 0.10 = 10%), summary regeneration is queued
- [ ] If `changeRatio <= threshold`, no regeneration (typos, minor formatting)
- [ ] If the page has no summary at all (new page or never generated), always trigger regardless of threshold
- [ ] The change detection runs synchronously as a quick check; the LLM call is async

### LLM Summary Generation
- [ ] The service sends the page's `plainText` (or full content if under 4000 chars) to an LLM
- [ ] Prompt produces two outputs:
  - **One-liner**: Max 100 characters. A few words describing the page's purpose. No period at the end.
  - **Summary**: Max 500 characters. 2-4 sentences covering the page's scope, key topics, and value.
- [ ] LLM provider is configurable: OpenAI (`gpt-4o-mini`) or Anthropic (`claude-3-haiku`)
- [ ] API key read from `SUMMARY_LLM_API_KEY` environment variable
- [ ] Provider selected via `SUMMARY_LLM_PROVIDER` (`openai` | `anthropic`, default: `openai`)
- [ ] Model selected via `SUMMARY_LLM_MODEL` (default: `gpt-4o-mini`)

### Async Non-Blocking Execution
- [ ] Page save (PUT) returns immediately — does NOT wait for LLM response
- [ ] Summary generation runs as an async background task (fire-and-forget with error logging)
- [ ] After LLM response, updates `Page.oneLiner`, `Page.summary`, `Page.summaryUpdatedAt`
- [ ] Triggers React Query cache invalidation for the page (via a revalidation mechanism or timestamp)

### Rate Limiting & Cost Control
- [ ] Maximum of `SUMMARY_RATE_LIMIT` LLM calls per minute per tenant (default: 10)
- [ ] If rate limit is exceeded, the regeneration is deferred (not dropped) — queued for next available slot
- [ ] Each LLM call is logged with: pageId, provider, model, input tokens, output tokens, latency, cost estimate
- [ ] Total LLM spend is trackable via sweep history (EPIC-34 will aggregate)

### Batch Generation CLI
- [ ] `npx skb generate-summaries --tenant <id>` generates summaries for all pages without one
- [ ] Supports `--overwrite` flag to regenerate all summaries regardless of staleness
- [ ] Supports `--dry-run` flag to show which pages would be processed
- [ ] Supports `--limit <n>` to cap the number of pages processed
- [ ] Respects rate limiting between LLM calls
- [ ] Prints progress: "Generated 15/42 summaries... (3 errors)"

### Graceful Degradation
- [ ] If `SUMMARY_LLM_API_KEY` is not set, summary generation is silently disabled
- [ ] No errors, no warnings in production logs — just a one-time info log at startup: "Summary generation disabled: no LLM API key configured"
- [ ] Manual summary editing (PUT /api/pages/{id}/summary) always works regardless of LLM config
- [ ] The "Regenerate with AI" button in the UI is disabled/hidden when no API key is configured

### Error Handling
- [ ] LLM timeout (30s): log error, do not update summary, do not retry immediately
- [ ] LLM API error (rate limit, server error): log, defer to next available slot
- [ ] Invalid LLM response (missing fields, too long): log, keep existing summary unchanged
- [ ] Network error: log, mark as failed, available for retry via sweep (EPIC-34)

---

## Architecture Overview

```
Change Detection Flow:
──────────────────────

PUT /api/pages/{id}/blocks
        │
        ▼
1. Save blocks to DB (existing logic)
2. Read oldPlainText from the previous Block.plainText
3. Compute newPlainText from updated content
4. Calculate changeRatio:
   ratio = approximateEditDistance(old, new) / max(old.length, new.length)
        │
        ├── ratio > 0.10 OR summary is null
        │   → Queue: summaryGenerationQueue.add(pageId)
        │
        └── ratio <= 0.10
            → Skip: log("Minor edit, skipping summary regen")
        │
        ▼
5. Return response to client (immediate, does not wait for LLM)


Summary Generation Pipeline:
─────────────────────────────

summaryGenerationQueue processes:
        │
        ▼
1. Check rate limit (tenant bucket: 10/min)
   - If available → proceed
   - If exhausted → defer (re-queue with delay)
        │
        ▼
2. Load page plainText and title
3. Construct LLM prompt:
   """
   Analyze the following page from a knowledge base.

   Title: {title}
   Content:
   {plainText (truncated to 4000 chars)}

   Generate:
   1. ONE-LINER (max 100 chars): A brief phrase describing what this page is about.
      Do not end with a period. Be specific, not generic.
      Good: "JWT authentication setup for REST API endpoints"
      Bad: "Information about authentication"

   2. SUMMARY (max 500 chars, 2-4 sentences): Describe the scope, key topics covered,
      and the value of this page. Be concrete.

   Respond in JSON: { "oneLiner": "...", "summary": "..." }
   """
        │
        ▼
4. Call LLM API (OpenAI or Anthropic)
5. Parse response JSON
6. Validate:
   - oneLiner.length <= 100
   - summary.length <= 500
   - Both are non-empty strings
        │
        ▼
7. Update Page record:
   prisma.page.update({
     where: { id: pageId },
     data: {
       oneLiner: result.oneLiner,
       summary: result.summary,
       summaryUpdatedAt: new Date(),
     }
   })
        │
        ▼
8. Log: { pageId, provider, model, inputTokens, outputTokens, latencyMs, estimatedCost }


Change Ratio Approximation:
───────────────────────────

For performance (runs on every save), we use a simplified diff:

function approximateChangeRatio(oldText: string, newText: string): number {
  if (oldText === newText) return 0;
  if (!oldText || !newText) return 1;  // Empty to content = 100%

  // Fast approximation: compare character frequency distributions
  // and sample-based comparison. Not full Levenshtein (too slow for
  // large documents on every save).

  const maxLen = Math.max(oldText.length, newText.length);
  const minLen = Math.min(oldText.length, newText.length);

  // Length difference contributes to ratio
  const lengthDiff = maxLen - minLen;

  // Sample character comparison (every 10th char)
  let sampleDiffs = 0;
  const sampleSize = Math.min(minLen, 200);
  const step = Math.max(1, Math.floor(minLen / sampleSize));
  for (let i = 0; i < minLen; i += step) {
    if (oldText[i] !== newText[i]) sampleDiffs++;
  }

  const sampleRatio = sampleDiffs / (sampleSize || 1);
  return Math.min(1, (lengthDiff / maxLen) + sampleRatio);
}

// This is a heuristic — exact Levenshtein is O(n*m) and too slow
// for documents with thousands of characters on every keystroke-debounced save.
```

---

## Implementation Steps

### Step 1: Create Summary Configuration

**File: `src/lib/summary/config.ts`** (create)

```typescript
export const SUMMARY_LLM_PROVIDER = process.env.SUMMARY_LLM_PROVIDER || 'openai';
export const SUMMARY_LLM_MODEL = process.env.SUMMARY_LLM_MODEL || 'gpt-4o-mini';
export const SUMMARY_LLM_API_KEY = process.env.SUMMARY_LLM_API_KEY || '';
export const SUMMARY_CHANGE_THRESHOLD = parseFloat(process.env.SUMMARY_CHANGE_THRESHOLD || '0.10');
export const SUMMARY_RATE_LIMIT = parseInt(process.env.SUMMARY_RATE_LIMIT || '10'); // per minute per tenant
export const SUMMARY_MAX_INPUT_CHARS = 4000;
export const ONE_LINER_MAX_LENGTH = 100;
export const SUMMARY_MAX_LENGTH = 500;

export function isSummaryGenerationEnabled(): boolean {
  return SUMMARY_LLM_API_KEY.length > 0;
}
```

### Step 2: Create Change Detection Module

**File: `src/lib/summary/changeDetection.ts`** (create)

```typescript
export function approximateChangeRatio(oldText: string, newText: string): number {
  // Fast character-level diff approximation
}

export function shouldRegenerateSummary(
  changeRatio: number,
  currentOneLiner: string | null,
  threshold: number
): boolean {
  // True if ratio > threshold OR no existing summary
}
```

### Step 3: Create LLM Provider Abstraction

**File: `src/lib/summary/llmProvider.ts`** (create)

```typescript
export interface LLMResponse {
  oneLiner: string;
  summary: string;
  inputTokens: number;
  outputTokens: number;
  latencyMs: number;
}

export interface LLMProvider {
  generateSummary(title: string, content: string): Promise<LLMResponse>;
}

export class OpenAIProvider implements LLMProvider { /* ... */ }
export class AnthropicProvider implements LLMProvider { /* ... */ }

export function createLLMProvider(): LLMProvider | null {
  if (!isSummaryGenerationEnabled()) return null;
  // Return appropriate provider based on config
}
```

### Step 4: Create Summary Generation Service

**File: `src/lib/summary/SummaryService.ts`** (create)

```typescript
export class SummaryService {
  private provider: LLMProvider | null;
  private rateLimiter: RateLimiter;

  constructor() {
    this.provider = createLLMProvider();
    this.rateLimiter = new RateLimiter(SUMMARY_RATE_LIMIT);
  }

  /**
   * Check if summary should be regenerated and queue if needed.
   * Called after every page save.
   */
  async onPageSaved(pageId: string, oldPlainText: string, newPlainText: string): Promise<void>;

  /**
   * Generate summary for a specific page (called by queue processor).
   */
  async generateForPage(pageId: string): Promise<void>;

  /**
   * Batch generate summaries for all pages missing them.
   */
  async generateBatch(tenantId: string, options: BatchOptions): Promise<BatchResult>;
}
```

### Step 5: Create LLM Prompt Templates

**File: `src/lib/summary/prompts.ts`** (create)

```typescript
export function buildSummaryPrompt(title: string, content: string): string {
  // Structured prompt that produces JSON { oneLiner, summary }
  // Include examples of good vs bad summaries
  // Truncate content to SUMMARY_MAX_INPUT_CHARS
}
```

### Step 6: Create Rate Limiter

**File: `src/lib/summary/rateLimiter.ts`** (create)

```typescript
export class RateLimiter {
  // Token bucket rate limiter
  // Per-tenant buckets
  // Configurable rate (default: 10/min)
  async acquire(tenantId: string): Promise<boolean>;
  async waitForSlot(tenantId: string): Promise<void>;
}
```

### Step 7: Hook into Page Save Flow

**File: `src/app/api/pages/[id]/blocks/route.ts`** (modify)

- After successful block update:
  - Read previous plainText
  - Compute new plainText
  - Call `summaryService.onPageSaved(pageId, oldPlainText, newPlainText)`

### Step 8: Create Batch Generation CLI

**File: `scripts/generate-summaries.ts`** (create)

```typescript
#!/usr/bin/env tsx
// Parse args: --tenant, --overwrite, --dry-run, --limit
// Instantiate SummaryService
// Call generateBatch with options
// Print progress and results
```

### Step 9: Enable "Regenerate with AI" Button

**File: `src/components/page/PageAboutSection.tsx`** (modify from SKB-33.1)

- Check if summary generation is enabled (API call or client-side env check)
- If enabled: "Regenerate with AI" button calls `POST /api/pages/{id}/summary/generate`
- If disabled: button hidden or shows "Configure LLM API key to enable"

### Step 10: Create Summary Generation Trigger API

**File: `src/app/api/pages/[id]/summary/generate/route.ts`** (create)

```typescript
// POST — Trigger summary regeneration for a specific page
export async function POST(req: NextRequest, { params }: { params: { id: string } }) {
  // 1. Authenticate
  // 2. Check if LLM is configured
  // 3. Call summaryService.generateForPage(pageId)
  // 4. Return updated summary
}
```

---

## Testing Requirements

### Unit Tests (10+ cases)

**File: `src/__tests__/lib/summary/changeDetection.test.ts`**

- Identical text → ratio 0.0
- Empty old, new has content → ratio 1.0
- Single character change in 100-char text → ratio ~0.01 (below threshold)
- 20 characters changed in 100-char text → ratio ~0.20 (above threshold)
- `shouldRegenerateSummary(0.05, "existing", 0.10)` → false
- `shouldRegenerateSummary(0.15, "existing", 0.10)` → true
- `shouldRegenerateSummary(0.05, null, 0.10)` → true (no existing summary)

**File: `src/__tests__/lib/summary/SummaryService.test.ts`**

- Page with big change → queues regeneration
- Page with minor change → skips regeneration
- Rate limiter respects per-minute limit
- LLM response parsed and validated correctly
- Invalid LLM response → keeps existing summary
- No API key configured → generation silently disabled

**File: `src/__tests__/lib/summary/prompts.test.ts`**

- Prompt includes title and truncated content
- Content over 4000 chars is truncated
- Prompt format is valid for both OpenAI and Anthropic

### Integration Tests (4+ cases)

**File: `src/__tests__/integration/summary-generation.test.ts`** (with mocked LLM)

- Save page with major edit → summary regenerated → new values in DB
- Save page with typo fix → summary NOT regenerated
- Batch generation processes all pages without summaries
- Rate limiting: 15 requests in quick succession → only 10 processed in first minute

---

## Files to Create/Modify

| File | Action | Description |
|------|--------|-------------|
| `src/lib/summary/config.ts` | Create | Summary generation configuration |
| `src/lib/summary/changeDetection.ts` | Create | Change ratio calculation |
| `src/lib/summary/llmProvider.ts` | Create | LLM provider abstraction (OpenAI/Anthropic) |
| `src/lib/summary/SummaryService.ts` | Create | Core summary generation service |
| `src/lib/summary/prompts.ts` | Create | LLM prompt templates |
| `src/lib/summary/rateLimiter.ts` | Create | Token bucket rate limiter |
| `src/lib/summary/types.ts` | Create | Summary-related types |
| `src/app/api/pages/[id]/summary/generate/route.ts` | Create | Manual regeneration trigger |
| `scripts/generate-summaries.ts` | Create | Batch generation CLI |
| `src/app/api/pages/[id]/blocks/route.ts` | Modify | Hook change detection into save flow |
| `src/components/page/PageAboutSection.tsx` | Modify | Enable regenerate button |
| `.env.example` | Modify | Add LLM configuration variables |
| `package.json` | Modify | Add `openai` and/or `@anthropic-ai/sdk` |
| Tests | Create | Unit + integration tests |

---

**Last Updated:** 2026-02-27
